# =========================
# CORE APP SETTINGS
# =========================
DATA_DIR=./data
INDEX_DIR=./indexes/faiss          # used only if VECTOR_DB=faiss
TOP_K=4
USE_MMR=true
TEMPERATURE=0.1

# =========================
# LLM (choose ONE path)
# =========================
# A) FREE / LOCAL via Ollama (default)
USE_LOCAL_LLM=true                 # set false to use a hosted LLM instead
LOCAL_LLM_MODEL=llama3             # e.g. llama3 | mistral | phi3:mini | qwen2:7b | gemma:7b
OLLAMA_BASE_URL=http://localhost:11434  # if remote: http://YOUR_HOST:11434

# B) HOSTED LLM (OpenAI, optional)
# If you flip USE_LOCAL_LLM=false, fill these:
OPENAI_API_KEY=
CHAT_MODEL=gpt-4o-mini

# =========================
# EMBEDDINGS (choose ONE path)
# =========================
# A) FREE / LOCAL via HuggingFace (default)
USE_LOCAL_EMBEDDINGS=true          # set false to use OpenAI embeddings
EMBEDDING_PROVIDER=huggingface
HF_EMBEDDING_MODEL=BAAI/bge-small-en-v1.5   # 384-d
EMBED_DIM=384                       # MUST match the model’s dimension
NORMALIZE_EMBEDDINGS=true

# B) HOSTED embeddings (OpenAI, optional)
# If you flip USE_LOCAL_EMBEDDINGS=false, fill these:
EMBEDDING_MODEL=text-embedding-3-small      # 1536-d (adjust EMBED_DIM if your code reads it)
# If your code requires manual dim: EMBED_DIM=1536

# =========================
# VECTOR DATABASE (choose ONE)
# =========================
# A) Qdrant (recommended; works local or cloud)
VECTOR_DB=qdrant

# - Local Qdrant (FREE; for local runs)
#   docker run -p 6333:6333 qdrant/qdrant:latest
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=
QDRANT_COLLECTION=techdocs

# - Qdrant Cloud (for Streamlit Cloud deployments)
#   If you use cloud, set the two lines below and keep VECTOR_DB=qdrant
# QDRANT_URL=https://YOUR_QDRANT_CLUSTER_URL
# QDRANT_API_KEY=YOUR_QDRANT_API_KEY
# QDRANT_COLLECTION=techdocs

# B) FAISS (fully local, no server). To use it:
# VECTOR_DB=faiss

# =========================
# RERANKER (optional quality boost; default OFF)
# =========================
USE_RERANKER=false                  # set true to enable
RERANKER_PROVIDER=local             # local | cohere | jina
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
RERANKER_DEVICE=cpu                 # cpu | cuda
RERANKER_TOP_N=12                   # how many to re-rank
RERANKER_RETURN_K=4                 # how many final chunks to keep
RERANKER_BATCH_SIZE=8

# (If you ever use a hosted reranker)
COHERE_API_KEY=
COHERE_RERANK_MODEL=rerank-english-v3.0

# =========================
# LOGGING
# =========================
LOG_DIR=./logs
LOG_LEVEL=INFO                      # DEBUG | INFO | WARNING | ERROR
LOG_ROTATION=time                   # time | size
LOG_WHEN=midnight
LOG_INTERVAL=1
LOG_BACKUPS=14
LOG_MAX_BYTES=10485760
LOG_JSON=false
LOG_CONSOLE=true

# =========================
# LANGSMITH / LANGCHAIN TRACING (optional)
# =========================
LANGCHAIN_TRACING_V2=false
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=
LANGCHAIN_PROJECT=knowledge-assistant

# Legacy compatibility (safe to leave)
LANGSMITH_TRACING=false
LANGSMITH_API_KEY=

# =========================
# SAVE_CORPUS (optional PDF rendering)
# =========================
# If Chrome isn’t auto-detected:
# macOS example:
# CHROME_PATH=/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome
CHROME_PATH=
